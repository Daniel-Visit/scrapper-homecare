#!/usr/bin/env python3.11
"""
Script para ejecutar scraping de Cruz Blanca con argumentos CLI.

Uso:
    python3.11 scripts/scrape.py --year 2025 --month FEBRERO
    python3.11 scripts/scrape.py --year 2025 --month ENERO --prestador "76190254-7 - SOLUCIONES..."
"""

import argparse
import asyncio
import sys
from pathlib import Path
from playwright.async_api import async_playwright

# Agregar directorio ra√≠z al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from scraper.cruzblanca import CruzBlancaScraper


async def main_async(year: str, month: str, prestador: str = None):
    """Ejecuta el scraper de forma as√≠ncrona."""
    
    print("=" * 60)
    print(f"SCRAPER CRUZ BLANCA - {month} {year}")
    print("=" * 60)
    print()
    
    # Par√°metros
    job_id = f"{month.lower()}_{year}_{int(asyncio.get_event_loop().time())}"
    params = {
        "year": year,
        "month": month.upper(),
        "prestador": prestador,
        "job_id": job_id
    }
    
    print(f"üìÖ Per√≠odo: {params['month']} {params['year']}")
    print(f"üè• Prestador: {params['prestador'] or 'TODOS'}")
    print()
    print("‚ö†Ô∏è  El navegador se abrir√° autom√°ticamente")
    print("‚ö†Ô∏è  Ingresa tu RUT, contrase√±a y resuelve el CAPTCHA")
    print("‚ö†Ô∏è  NO CIERRES LA VENTANA MANUALMENTE")
    print()
    input("Presiona ENTER para continuar...")
    print()
    
    async with async_playwright() as p:
        # Lanzar navegador
        browser = await p.chromium.launch(
            headless=False,
            slow_mo=200
        )
        
        context = await browser.new_context(
            accept_downloads=True
        )
        
        page = await context.new_page()
        
        # Crear scraper
        scraper = CruzBlancaScraper(output_dir=Path("data"))
        scraper.context = context
        scraper.page = page
        scraper.job_id = params["job_id"]
        
        try:
            # Login manual
            await scraper.login_via_context(page, "manual", "manual")
            
            # Scraping
            all_pdfs = await scraper.discover_documents(page, params)
            
            print(f"\n‚úÖ PROCESO COMPLETADO")
            print(f"üìä Total PDFs descargados: {len(all_pdfs)}")
            print(f"üìÇ Ubicaci√≥n: data/pdfs/{params['job_id']}/")
            print(f"üìÑ Metadata: data/results/{params['job_id']}_results.json")
            
            return len(all_pdfs)
            
        except Exception as e:
            print(f"\n‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            return 0
        finally:
            await browser.close()


def main():
    """Funci√≥n principal con argumentos CLI."""
    parser = argparse.ArgumentParser(
        description="Ejecuta scraping de Cruz Blanca para un per√≠odo espec√≠fico"
    )
    parser.add_argument(
        "--year",
        required=True,
        help="A√±o a scrapear (ej: 2025)"
    )
    parser.add_argument(
        "--month",
        required=True,
        help="Mes a scrapear (ej: ENERO, FEBRERO, etc.)"
    )
    parser.add_argument(
        "--prestador",
        default=None,
        help="RUT y nombre del prestador (opcional)"
    )
    
    args = parser.parse_args()
    
    # Ejecutar scraping
    try:
        total_pdfs = asyncio.run(main_async(
            year=args.year,
            month=args.month,
            prestador=args.prestador
        ))
        
        sys.exit(0 if total_pdfs > 0 else 1)
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Proceso interrumpido por el usuario")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Error durante el scraping: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
