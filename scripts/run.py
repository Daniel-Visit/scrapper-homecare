#!/usr/bin/env python3
"""
Script orquestador del proceso completo: scraping ‚Üí validaci√≥n ‚Üí extracci√≥n.

Uso:
    # Proceso completo
    python scripts/run_process.py --year 2025 --month ENERO
    
    # Solo scraping
    python scripts/run_process.py --year 2025 --month ENERO --skip-extraction
    
    # Solo extracci√≥n (requiere scraping previo)
    python scripts/run_process.py --year 2025 --month ENERO --skip-scraping
    
    # Con prestador espec√≠fico
    python scripts/run_process.py --year 2025 --month ENERO --prestador "76190254-7 - SOLUCIONES..."
"""

import argparse
import getpass
import sys
from pathlib import Path

# Agregar directorio ra√≠z al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from scraper.models import ScrapingParams
from scraper.orchestrator import ProcessOrchestrator


def main():
    """Funci√≥n principal."""
    parser = argparse.ArgumentParser(
        description="Ejecuta el proceso completo de scraping y extracci√≥n de Cruz Blanca"
    )
    parser.add_argument(
        "--year",
        required=True,
        help="A√±o a procesar (ej: 2025)"
    )
    parser.add_argument(
        "--month",
        required=True,
        help="Mes a procesar (ej: ENERO, FEBRERO, etc.)"
    )
    parser.add_argument(
        "--prestador",
        default=None,
        help="RUT y nombre del prestador (opcional, si no se especifica procesa todos)"
    )
    parser.add_argument(
        "--skip-scraping",
        action="store_true",
        help="Salta el scraping (usa datos existentes)"
    )
    parser.add_argument(
        "--skip-extraction",
        action="store_true",
        help="Salta la extracci√≥n (solo scraping)"
    )
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Directorio base para datos (default: data)"
    )
    
    args = parser.parse_args()
    
    # Validar combinaciones
    if args.skip_scraping and args.skip_extraction:
        print("‚ùå Error: No se puede saltar scraping y extracci√≥n al mismo tiempo")
        sys.exit(1)
    
    # No solicitar credenciales - se ingresan manualmente en el navegador
    username = "manual"  # Placeholder - el usuario ingresar√° en el navegador
    password = "manual"  # Placeholder - el usuario ingresar√° en el navegador
    
    if not args.skip_scraping:
        print("=" * 60)
        print("‚ö†Ô∏è  INSTRUCCIONES")
        print("=" * 60)
        print("1. El navegador se abrir√° autom√°ticamente")
        print("2. Ingresa tu RUT y contrase√±a en el sitio web")
        print("3. Resuelve el CAPTCHA")
        print("4. El script detectar√° el login y continuar√° autom√°ticamente")
        print("=" * 60)
        input("\n‚úÖ Presiona ENTER cuando est√©s listo para continuar...")
        print()
    
    print("=" * 60)
    print("PAR√ÅMETROS DEL PROCESO")
    print("=" * 60)
    print(f"üìÖ A√±o: {args.year}")
    print(f"üìÖ Mes: {args.month.upper()}")
    print(f"üè• Prestador: {args.prestador or 'TODOS'}")
    print(f"üìÇ Directorio datos: {args.data_dir}")
    
    if args.skip_scraping:
        print("‚è≠Ô∏è  Modo: Solo extracci√≥n (scraping omitido)")
    elif args.skip_extraction:
        print("‚è≠Ô∏è  Modo: Solo scraping (extracci√≥n omitida)")
    else:
        print("üîÑ Modo: Proceso completo")
    
    print()
    
    # Crear par√°metros
    params = ScrapingParams(
        year=args.year,
        month=args.month.upper(),
        prestador=args.prestador,
        username=username or "",
        password=password or ""
    )
    
    # Ejecutar proceso
    orchestrator = ProcessOrchestrator(data_dir=args.data_dir)
    
    try:
        result = orchestrator.run_full_process(
            params=params,
            skip_scraping=args.skip_scraping,
            skip_extraction=args.skip_extraction
        )
        
        # Mostrar resultado
        print("\n" + "=" * 60)
        print("üìä RESULTADO FINAL")
        print("=" * 60)
        
        status = result.get("status")
        
        if status == "completed":
            print("‚úÖ Proceso completado exitosamente")
            
            scraping = result.get("scraping")
            if scraping:
                print(f"\nüì• SCRAPING:")
                print(f"   Job ID: {scraping['job_id']}")
                print(f"   PDFs descargados: {scraping['successful']}")
                print(f"   Tasa de √©xito: {scraping['validation']['success_rate']}%")
            
            extraction = result.get("extraction")
            if extraction:
                print(f"\nüì§ EXTRACCI√ìN:")
                print(f"   Archivos procesados: {extraction['total_files']}")
                print(f"   Extra√≠dos: {extraction['extracted']}")
                print(f"   Tasa de √©xito: {extraction['success_rate']}%")
                print(f"   Archivo salida: {extraction['output_file']}")
            
            sys.exit(0)
            
        elif status == "validation_failed":
            print("‚ùå Proceso detenido por validaci√≥n fallida")
            
            scraping = result.get("scraping")
            if scraping:
                print(f"\n‚ö†Ô∏è  SCRAPING:")
                print(f"   Tasa de √©xito: {scraping['validation']['success_rate']}%")
                print(f"   PDFs fallidos: {scraping['failed']}")
                print(f"   ‚ùå No se cumplieron los criterios de calidad")
            
            sys.exit(1)
        
        else:
            print(f"‚ö†Ô∏è  Estado desconocido: {status}")
            sys.exit(1)
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Proceso interrumpido por el usuario")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Error durante el proceso: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

