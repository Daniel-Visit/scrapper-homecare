# ğŸ¥ Scraper Cruz Blanca

Sistema modular para scraping y extracciÃ³n de datos de Isapres chilenas, con **validaciÃ³n robusta** que garantiza 100% de completitud.

## âœ¨ CaracterÃ­sticas Principales

- âœ… **ValidaciÃ³n automÃ¡tica** - Verifica que todos los PDFs se descargaron correctamente
- ğŸ”„ **Reintentos inteligentes** - Recupera automÃ¡ticamente descargas fallidas  
- ğŸ“Š **MÃ©tricas detalladas** - Reportes completos de Ã©xito/fallo
- ğŸ§© **MÃ³dulos desacoplados** - Scraping y extracciÃ³n independientes
- ğŸ¯ **Proceso parametrizable** - Mes, aÃ±o y prestador configurables

---

## ğŸš€ Inicio RÃ¡pido

### InstalaciÃ³n

```bash
# Instalar dependencias
pip install -r requirements.txt

# Instalar navegador Chromium
python -m playwright install chromium
```

### Uso BÃ¡sico

```bash
# Proceso completo (scraping + validaciÃ³n + extracciÃ³n)
python scripts/run.py --year 2025 --month ENERO

# Solo scraping
python scripts/scrape.py --year 2025 --month ENERO

# Solo extracciÃ³n (requiere scraping previo)
python scripts/extract.py --input data/pdfs/enero_2025_xxx/ --output data/json/enero_2025.json
```

---

## ğŸ“Š MÃ©tricas de Ã‰xito

El sistema genera un **reporte de validaciÃ³n completo** despuÃ©s de cada scraping:

```json
{
  "validation": {
    "passed": true,                    // âœ… TRUE solo si tasa >= 95% y sin corruptos
    "total_expected": 150,             // PDFs que debÃ­an descargarse
    "total_downloaded": 150,           // PDFs descargados exitosamente
    "success_rate": 100.0,             // Porcentaje de Ã©xito
    "failed_records": [],              // Registros fallidos con detalles
    "corrupted_files": [],             // Archivos corruptos detectados
    "retry_successes": 5,              // PDFs recuperados en reintentos
    "total_size_bytes": 45678900       // TamaÃ±o total descargado
  }
}
```

### Criterios de ValidaciÃ³n

El proceso **solo continÃºa con extracciÃ³n** si:
- âœ… Tasa de Ã©xito >= 95%
- âœ… Sin archivos corruptos (< 1KB)
- âœ… Todos los registros tienen PDF asociado

Si la validaciÃ³n falla, el sistema se detiene y genera un reporte detallado.

---

## ğŸ“‚ Estructura del Proyecto

```
scrapper-mvp/
â”œâ”€â”€ scraper/                   # ğŸ§© MÃ³dulos de scraping y extracciÃ³n
â”‚   â”œâ”€â”€ cruzblanca.py         # Scraper Cruz Blanca (incluye selectores)
â”‚   â”œâ”€â”€ isapre_x.py           # Placeholder para otros scrapers
â”‚   â”œâ”€â”€ extractor.py          # ExtracciÃ³n de datos de PDFs
â”‚   â”œâ”€â”€ orchestrator.py       # Coordinador del proceso completo
â”‚   â”œâ”€â”€ models.py             # Modelos de datos (Pydantic)
â”‚   â””â”€â”€ base.py               # Clase base para scrapers
â”œâ”€â”€ scripts/                   # ğŸ¯ Scripts ejecutables
â”‚   â”œâ”€â”€ scrape.py             # Scraping independiente
â”‚   â”œâ”€â”€ extract.py            # ExtracciÃ³n independiente
â”‚   â””â”€â”€ run.py                # Proceso completo orquestado
â”œâ”€â”€ data/                      # ğŸ’¾ Datos generados
â”‚   â”œâ”€â”€ pdfs/                 # PDFs descargados (por job_id)
â”‚   â”œâ”€â”€ json/                 # JSONs extraÃ­dos
â”‚   â””â”€â”€ logs/                 # Logs, cookies y reportes
â”œâ”€â”€ docs/                      # ğŸ“š DocumentaciÃ³n tÃ©cnica
â”‚   â”œâ”€â”€ exploracion/          # ExploraciÃ³n del sitio Cruz Blanca
â”‚   â”œâ”€â”€ guias/                # GuÃ­as de implementaciÃ³n
â”‚   â””â”€â”€ evidence/             # Evidencia y reportes
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                  # Este archivo
```

---

## ğŸ”§ Uso Detallado

### 1. Proceso Completo (Recomendado)

Ejecuta scraping â†’ validaciÃ³n â†’ extracciÃ³n automÃ¡ticamente:

```bash
python scripts/run.py --year 2025 --month ENERO
```

**Con prestador especÃ­fico:**
```bash
python scripts/run.py --year 2025 --month ENERO --prestador "76190254-7 - SOLUCIONES INTEGRALES"
```

**Saltar etapas:**
```bash
# Solo scraping (sin extracciÃ³n)
python scripts/run.py --year 2025 --month ENERO --skip-extraction

# Solo extracciÃ³n (requiere scraping previo)
python scripts/run.py --year 2025 --month ENERO --skip-scraping
```

### 2. Scraping Independiente

```bash
python scripts/scrape.py --year 2025 --month ENERO
```

**Entrada:**
- RUT del usuario (solicitado interactivamente)
- ContraseÃ±a (solicitada interactivamente)
- Resolver CAPTCHA manualmente en el navegador

**Salida:**
- PDFs: `data/pdfs/enero_2025_YYYYMMDD_HHMMSS/`
- Metadata: `data/results/enero_2025_YYYYMMDD_HHMMSS_results.json`
- Reporte de validaciÃ³n incluido en metadata

### 3. ExtracciÃ³n Independiente

```bash
python scripts/extract.py \
  --input data/pdfs/enero_2025_20251021_143022/ \
  --output data/json/enero_2025_extracted.json \
  --metadata data/results/enero_2025_20251021_143022_results.json
```

**Salida:**
- JSON estructurado con datos extraÃ­dos
- ValidaciÃ³n de completitud

---

## ğŸ” Proceso de Login

1. El script solicita **RUT y contraseÃ±a**
2. Se abre el navegador en modo visible (no headless)
3. **Usuario resuelve CAPTCHA manualmente** âš ï¸
4. Script detecta login exitoso automÃ¡ticamente
5. ContinÃºa con el scraping

**Nota:** El CAPTCHA debe resolverse manualmente (no se puede automatizar por ToS de Google).

---

## ğŸ“– Arquitectura

### MÃ³dulos Desacoplados

El sistema estÃ¡ diseÃ±ado con mÃ³dulos independientes que se pueden ejecutar por separado:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scraping  â”‚ â”€â”€> â”‚  ValidaciÃ³n  â”‚ â”€â”€> â”‚ ExtracciÃ³n  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“                    â†“                     â†“
  Descarga PDFs      Verifica 100%        Genera JSON
```

### Flujo del Proceso

1. **Scraping** - `scraper/cruzblanca.py`
   - Login con CAPTCHA manual
   - NavegaciÃ³n y filtros
   - Descarga masiva de PDFs
   - PaginaciÃ³n automÃ¡tica
   - Reintentos en fallos

2. **ValidaciÃ³n** - AutomÃ¡tica
   - Cuenta PDFs esperados vs descargados
   - Verifica integridad (tamaÃ±o > 1KB)
   - Valida que cada registro tiene PDF
   - Genera reporte detallado

3. **ExtracciÃ³n** - `scraper/extractor.py`
   - Lee PDFs descargados
   - Extrae datos estructurados
   - Genera JSON con metadata
   - **Solo se ejecuta si validaciÃ³n pasa**

4. **OrquestaciÃ³n** - `scraper/orchestrator.py`
   - Coordina todo el proceso
   - Maneja flujo condicional
   - Genera reportes finales

---

## ğŸ¯ Casos de Uso

### Caso 1: Scraping de Enero 2025 Completo

```bash
# Ejecutar proceso completo
python scripts/run.py --year 2025 --month ENERO

# Input manual:
ğŸ‘¤ Ingresa tu RUT: 12345678-9
ğŸ”‘ Ingresa tu contraseÃ±a: ****

# El navegador se abre â†’ Resolver CAPTCHA
# Script continÃºa automÃ¡ticamente

# Output:
ğŸ“¥ SCRAPING:
   Job ID: enero_2025_20251021_143022
   PDFs descargados: 156
   Tasa de Ã©xito: 100.0%

ğŸ¯ VALIDACIÃ“N: âœ… EXITOSA

ğŸ“¤ EXTRACCIÃ“N:
   Archivos procesados: 156
   ExtraÃ­dos: 156
   Tasa de Ã©xito: 100.0%
   Archivo salida: data/json/enero_2025_20251021_143022_extracted.json
```

### Caso 2: Re-extraer con Mejores ParÃ¡metros

Si ya tienes los PDFs descargados y quieres re-extraer:

```bash
python scripts/extract.py \
  --input data/pdfs/enero_2025_20251021_143022/ \
  --output data/json/enero_2025_v2.json
```

### Caso 3: Validar Scraping Existente

Los resultados de scraping incluyen el reporte de validaciÃ³n:

```bash
cat data/results/enero_2025_20251021_143022_results.json | jq '.validation'
```

---

## ğŸ“š DocumentaciÃ³n Adicional

### ExploraciÃ³n del Sitio

- [Pasos de ExploraciÃ³n de Cruz Blanca](docs/exploracion/pasos_cruzblanca.md) - CÃ³mo funciona el sitio, selectores, flujo de navegaciÃ³n
- [Hallazgos de ExploraciÃ³n](docs/exploracion/hallazgos.md) - Descubrimientos importantes, quirks del sitio

### GuÃ­as TÃ©cnicas

- [GuÃ­a de ImplementaciÃ³n MVP](docs/guias/implementacion_mvp.md) - Arquitectura del sistema, decisiones de diseÃ±o

### Evidencia

- [Reporte Sistema Integrado](docs/evidence/sistema_integrado.md) - Evidencia de pruebas y resultados

---

## ğŸ†˜ Troubleshooting

### Error: "ValidaciÃ³n fallida"

**SÃ­ntomas:** El proceso se detiene despuÃ©s del scraping con mensaje de validaciÃ³n fallida.

**Causas posibles:**
- ConexiÃ³n interrumpida durante descarga
- PDFs corruptos
- Timeouts en el sitio

**SoluciÃ³n:**
1. Revisar reporte en `data/results/[job_id]_results.json`
2. Ver `validation.failed_records` para identificar problemas
3. Re-ejecutar el scraping para ese perÃ­odo

### Error: "No se encontraron PDFs"

**Causas posibles:**
- PerÃ­odo sin datos en Cruz Blanca
- Prestador incorrectamente escrito
- Credenciales invÃ¡lidas

**SoluciÃ³n:**
1. Verificar que el perÃ­odo tiene datos en el sitio
2. Verificar formato del prestador: `"RUT - NOMBRE COMPLETO"`
3. Verificar credenciales

### CAPTCHA no se resuelve

**SoluciÃ³n:**
- El navegador debe permanecer abierto y visible
- Resolver el CAPTCHA manualmente
- El script esperarÃ¡ automÃ¡ticamente (hasta 5 minutos)

---

## ğŸ”® Roadmap

- [ ] API REST para gatillar procesos remotamente
- [ ] IntegraciÃ³n SFTP a Digital Ocean
- [ ] ExtracciÃ³n avanzada con OCR para PDFs escaneados
- [ ] Scrapers para otras Isapres
- [ ] Scheduling automÃ¡tico con cron
- [ ] Dashboard de monitoreo

---

## ğŸ¤ Contribuciones

Para agregar un nuevo scraper:

1. Crear `scraper/nueva_isapre.py` heredando de `ScraperBase`
2. Implementar mÃ©todos requeridos
3. Agregar a `scraper/__init__.py`
4. Documentar selectores y flujo

---

## ğŸ“ Notas Importantes

- âš ï¸ **CAPTCHA manual obligatorio** - No se puede automatizar
- âœ… **ValidaciÃ³n crÃ­tica** - No se extrae si validaciÃ³n falla
- ğŸ“Š **MÃ©tricas siempre disponibles** - En archivos `_results.json`
- ğŸ”„ **Reintentos automÃ¡ticos** - El scraper reintenta fallos automÃ¡ticamente
- ğŸ¯ **Tasa objetivo: 95%+** - Criterio de Ã©xito estricto

---

## ğŸ“„ Licencia

[Especificar licencia]

## ğŸ“ Contacto

Para dudas o problemas, revisar logs en `data/logs/` o consultar la documentaciÃ³n en `docs/`.
