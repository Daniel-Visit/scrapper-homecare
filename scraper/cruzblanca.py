"""
Plugin de scraping para Cruz Blanca Extranet.
Implementa el flujo completo: login ‚Üí navegaci√≥n ‚Üí descarga de PDFs.
"""

import asyncio
import csv
import json
import os
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from playwright.async_api import Page, BrowserContext, Frame, Download

from scraper.base import ScraperBase
from config.cruzblanca_selectors import (
    LOGIN_URL, LOGIN_SELECTORS, DASHBOARD_SELECTORS, 
    PRESTADORES_SUBMENU, IFRAME_SELECTOR, CONTROL_RECEPCION_SELECTORS
)


class DebugLogger:
    """Sistema de logging aut√≥nomo con screenshots y HTML dumps."""
    
    def __init__(self, job_id: str):
        self.debug_dir = Path("data/debug") / job_id
        self.debug_dir.mkdir(parents=True, exist_ok=True)
        self.events = []
    
    async def capture_state(self, page: Page, iframe: Frame, step_name: str, extra_data: dict = None):
        """Captura screenshot, HTML y datos del estado actual."""
        timestamp = time.time()
        
        try:
            # Screenshot
            screenshot = self.debug_dir / f"{step_name}_{timestamp:.0f}.png"
            await page.screenshot(path=screenshot)
            
            # HTML dump
            html_file = self.debug_dir / f"{step_name}_{timestamp:.0f}.html"
            try:
                html_content = await iframe.locator("#panelResumen_CallBackPanel_1").inner_html()
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(html_content)
            except Exception as e:
                html_file = None
            
            # Event log
            event = {
                "timestamp": timestamp,
                "step": step_name,
                "screenshot": screenshot.name,
                "html": html_file.name if html_file else None,
                "data": extra_data or {}
            }
            self.events.append(event)
            
            return event
        except Exception as e:
            # Si falla la captura, al menos registrar el error
            event = {
                "timestamp": timestamp,
                "step": step_name,
                "error": str(e),
                "data": extra_data or {}
            }
            self.events.append(event)
            return event
    
    def save_report(self):
        """Guarda el reporte completo de eventos."""
        report_file = self.debug_dir / "debug_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.events, f, indent=2)
    
    def analyze_and_save(self):
        """Analiza el reporte y genera conclusiones autom√°ticas."""
        if not self.events:
            return None
        
        analysis = {
            "total_eventos": len(self.events),
            "enlaces_procesados": 0,
            "errores": [],
            "cambios_tabla": [],
            "pdfs_totales": 0
        }
        
        # Detectar cambios en n√∫mero de enlaces
        prev_count = None
        for event in self.events:
            if "error" in event:
                analysis["errores"].append({
                    "step": event["step"],
                    "error": event["error"]
                })
            
            if "enlaces_frescos" in event.get("data", {}):
                current = event["data"]["enlaces_frescos"]
                if prev_count is not None and current != prev_count:
                    analysis["cambios_tabla"].append({
                        "step": event["step"],
                        "anterior": prev_count,
                        "actual": current,
                        "diferencia": current - prev_count
                    })
                prev_count = current
            
            if "pdfs_descargados" in event.get("data", {}):
                analysis["pdfs_totales"] += event["data"]["pdfs_descargados"]
                analysis["enlaces_procesados"] += 1
        
        # Guardar an√°lisis
        analysis_file = self.debug_dir / "analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2)
        
        return analysis


class CruzBlancaScraper(ScraperBase):
    """Scraper para Cruz Blanca Extranet."""
    
    site_id = "cruzblanca"
    
    def __init__(self, output_dir: Path = None):
        super().__init__()
        self.output_dir = output_dir or Path("data")
        self.pdf_dir = self.output_dir / "pdfs"
        self.results_dir = self.output_dir / "results"
        self.logs_dir = self.output_dir / "logs"
        
        # Crear directorios
        self.pdf_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.logs_dir.mkdir(parents=True, exist_ok=True)
        
        # Variables de estado
        self.job_id = None
        self.context = None
        self.page = None
        self.iframe = None
        self.downloads = []
        self.logger = None  # Se inicializa cuando se establece job_id
        
    async def login_via_context(self, page: Page, username: str, password: str) -> None:
        """Realiza login en Cruz Blanca Extranet."""
        print(f"üîê Iniciando proceso de login...")
        
        # Navegar a login
        await page.goto(LOGIN_URL, wait_until="networkidle", timeout=60000)
        print("‚úÖ P√°gina de login cargada")
        
        # IMPORTANTE: El usuario hace login manualmente
        print("‚ö†Ô∏è  INICIA SESI√ìN MANUALMENTE en el navegador:")
        print("   1. Ingresa tu RUT y contrase√±a")
        print("   2. Resuelve el CAPTCHA")
        print("   3. Presiona el bot√≥n de login")
        print("   El script detectar√° autom√°ticamente cuando hayas iniciado sesi√≥n...")
        print()
        
        # Esperar a que el usuario haga login manualmente
        # Detectamos que ya inici√≥ sesi√≥n cuando la URL cambia a Extranet.aspx
        try:
            # Esperar hasta 5 minutos a que aparezca el dashboard (tiempo suficiente para login manual)
            await page.wait_for_url("**/Extranet.aspx", timeout=300000)
            print("‚úÖ Login exitoso - Dashboard detectado")
            
            # Esperar a que cargue completamente
            await page.wait_for_load_state("networkidle", timeout=30000)
            
            # Intentar obtener el nombre del usuario
            try:
                user_name = await page.text_content(DASHBOARD_SELECTORS["user_name"])
                if user_name:
                    print(f"üë§ Usuario: {user_name}")
            except:
                print("üë§ Dashboard cargado")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Timeout esperando el dashboard: {e}")
            print("‚ö†Ô∏è  Aseg√∫rate de haber iniciado sesi√≥n correctamente")
            raise
        
        # Guardar cookies para reutilizar
        if self.context:
            cookies = await self.context.cookies()
            cookies_file = self.logs_dir / f"{self.job_id}_cookies.json"
            cookies_file.write_text(json.dumps(cookies, indent=2))
            print(f"üç™ Cookies guardadas en: {cookies_file}")
        else:
            print("‚ö†Ô∏è  Contexto no disponible para guardar cookies")
    
    async def navigate_to_accounts_section(self, page: Page = None) -> None:
        """Navega a la secci√≥n de Control Recepci√≥n Cuentas Electr√≥nicas."""
        print("üß≠ Navegando a Control Recepci√≥n Cuentas Electr√≥nicas...")
        
        if page:
            self.page = page
        
        if not self.page:
            raise Exception("‚ùå Page no est√° inicializada")
        
        # Hacer hover sobre men√∫ PRESTADORES para desplegar submenu
        print("üîÑ Haciendo hover en men√∫ Prestadores...")
        await self.page.hover(DASHBOARD_SELECTORS["menu_prestadores"])
        await asyncio.sleep(3)  # Esperar a que se despliegue
        
        # Verificar que el submenu es visible
        print("üîç Verificando submenu...")
        try:
            await self.page.wait_for_selector(PRESTADORES_SUBMENU["control_recepcion"], state="visible", timeout=10000)
            print("‚úÖ Submenu visible")
        except Exception as e:
            print(f"‚ö†Ô∏è  Submenu no visible: {e}")
            # Intentar click directo en el men√∫ principal
            await self.page.click(DASHBOARD_SELECTORS["menu_prestadores"])
            await asyncio.sleep(2)
        
        # Click en Control Recepci√≥n Cuentas Electr√≥nicas
        print("üîÑ Haciendo click en Control Recepci√≥n...")
        await self.page.click(PRESTADORES_SUBMENU["control_recepcion"])
        await asyncio.sleep(5)  # Esperar sin networkidle
        
        # Esperar a que cargue el iframe
        await self.page.wait_for_selector(IFRAME_SELECTOR, timeout=30000)
        
        # Cambiar contexto al iframe
        self.iframe = self.page.frame_locator(IFRAME_SELECTOR)
        print("‚úÖ Navegaci√≥n completada - iframe cargado")
    
    async def apply_filters(self, prestador: str = None, year: str = "2025", month: str = "SEPTIEMBRE") -> None:
        """Aplica filtros usando los selectores exactos de Playwright codegen."""
        print(f"üîç Aplicando filtros: Prestador={prestador}, A√±o={year}, Mes={month}")
        
        # Esperar a que cargue el iframe
        await asyncio.sleep(2)
        
        # Seleccionar prestador
        if prestador:
            try:
                # Click en el input del prestador (selector correcto del codegen)
                await self.iframe.locator("#cmbEntidades_I").click()
                await asyncio.sleep(1)
                
                # Click en la opci√≥n del prestador (buscar por el texto completo)
                await self.iframe.get_by_role("cell", name=prestador, exact=True).click()
                print(f"  ‚úì Prestador seleccionado")
                await asyncio.sleep(2)
            except Exception as e:
                print(f"  ‚ö†Ô∏è  No se pudo seleccionar prestador exacto, continuando: {e}")
        
        # Seleccionar a√±o
        try:
            # Click en el input del a√±o (selector correcto del codegen)
            await self.iframe.locator("#cmdAgnos_I").click()
            await asyncio.sleep(1)
            
            # Click en la opci√≥n del a√±o
            await self.iframe.get_by_role("cell", name=year, exact=True).click()
            print(f"  ‚úì A√±o {year} seleccionado")
            await asyncio.sleep(1)
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error seleccionando a√±o: {e}")
        
        # Seleccionar mes - con scroll JavaScript en contenedor DevExpress
        try:
            # Paso 1: Abrir dropdown
            print(f"  üîΩ Abriendo dropdown de meses...")
            await self.iframe.locator("#cmdMeses").get_by_role("cell").first.click()
            await asyncio.sleep(1)
            
            # Paso 2: Mapear mes a √≠ndice
            month_ids = {
                "ENERO": 0, "FEBRERO": 1, "MARZO": 2, "ABRIL": 3,
                "MAYO": 4, "JUNIO": 5, "JULIO": 6, "AGOSTO": 7,
                "SEPTIEMBRE": 8, "OCTUBRE": 9, "NOVIEMBRE": 10, "DICIEMBRE": 11
            }
            month_index = month_ids.get(month, 0)
            
            # Paso 3: Hacer scroll inteligente seg√∫n rango del mes
            if month_index <= 6:  # ENERO-JULIO (visibles desde el tope)
                print(f"  üìú Scroll al tope (meses ENERO-JULIO)...")
                await self.iframe.locator("#cmdMeses_DDD_L_D").evaluate("el => el.scrollTop = 0")
            else:  # AGOSTO-DICIEMBRE (visibles desde el fondo)
                print(f"  üìú Scroll al fondo (meses AGOSTO-DICIEMBRE)...")
                await self.iframe.locator("#cmdMeses_DDD_L_D").evaluate("el => el.scrollTop = el.scrollHeight")
            await asyncio.sleep(0.5)
            
            # Paso 4: Click en el mes por su ID fijo
            month_id = f"#cmdMeses_DDD_L_LBI{month_index}T0"
            print(f"  üîç Seleccionando {month}...")
            await self.iframe.locator(month_id).click()
            print(f"  ‚úì Click en {month}")
            await asyncio.sleep(0.5)
            
            # Paso 5: Cerrar dropdown para confirmar
            print(f"  üîí Cerrando dropdown...")
            await self.iframe.locator("#cmdMeses").get_by_role("img", name="v").click()
            print(f"  ‚úÖ Mes {month} confirmado")
            await asyncio.sleep(1)
        except Exception as e:
            print(f"  ‚ùå Error seleccionando mes: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        # Click en bot√≥n Consultar (selector correcto del codegen)
        try:
            await self.iframe.get_by_role("cell", name="Consultar Consultar", exact=True).locator("span").click()
            print(f"  ‚úì Bot√≥n Consultar presionado")
            await asyncio.sleep(5)
            
            # Verificar si hay resultados o mensaje de "No existen datos"
            # Ignorar mensaje "No existe Cuentas M√©dicas" - es falso positivo com√∫n
            if await self.iframe.locator("text=No existe Cuentas M√©dicas para el periodo consultado").count() > 0:
                print(f"‚ö†Ô∏è  Detectado mensaje 'No existe Cuentas M√©dicas' - IGNORANDO (falso positivo)")
                print(f"‚úÖ Continuando con el flujo normal...")
            
            # Solo verificar mensajes realmente cr√≠ticos
            no_data_messages = [
                "No se encontraron resultados",
                "Sin datos disponibles",
                "No hay informaci√≥n"
            ]
            
            for msg in no_data_messages:
                if await self.iframe.locator(f"text={msg}").count() > 0:
                    print(f"‚ö†Ô∏è  {msg}")
                    print(f"‚ö†Ô∏è  El per√≠odo {year}/{month} no tiene datos para este prestador")
                    return False  # Indicar que no hay datos
            
            print("‚úÖ Filtros aplicados correctamente")
            return True  # Hay datos
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error presionando Consultar: {e}")
            # Intento alternativo
            await self.iframe.locator("#btnConsultar").click()
            await asyncio.sleep(5)
            return True  # Asumir que hay datos si no podemos verificar
    
    async def get_summary_table_data_and_process(self) -> List[Dict[str, Any]]:
        """Obtiene datos de la tabla resumen, ordena y procesa INMEDIATAMENTE cada enlace."""
        print("üìä Procesando tabla resumen...")
        
        # Ordenar por "Cuentas A Pago" (descendente)
        await self.iframe.get_by_text("Cuentas A Pago").click()
        await asyncio.sleep(1)
        await self.iframe.get_by_text("Cuentas A Pago").click()
        await asyncio.sleep(2)
        
        all_pdfs = []
        page_num = 1
        
        while True:
            # Capturar estado inicial
            await self.logger.capture_state(self.page, self.iframe, f"tabla_resumen_pag{page_num}", {
                "pagina": page_num
            })
            
            # Buscar enlaces SOLO en columna "Cuentas A Pago"
            links = await self.iframe.locator("div[id*='panelAPago'] a[onclick*=\"DetalleCtas('APago'\"]").all()
            
            # Extraer datos con FECHA como identificador
            found_zero = False
            links_data = []
            
            for idx, link in enumerate(links):
                link_text = await link.text_content()
                onclick = await link.get_attribute("onclick")
                
                # Extraer fecha del onclick
                fecha_match = re.search(r"DetalleCtas\('APago',\s*'([^']+)'", onclick)
                fecha = fecha_match.group(1) if fecha_match else None
                
                if not fecha:
                    await self.logger.capture_state(self.page, self.iframe, f"error_fecha_idx{idx}", {
                        "onclick": onclick,
                        "text": link_text.strip() if link_text else ""
                    })
                    continue
                
                if link_text and link_text.strip().isdigit():
                    count = int(link_text.strip())
                    if count == 0:
                        found_zero = True
                        break
                    if count > 0:
                        links_data.append({
                            "fecha": fecha,  # Usar fecha como ID √∫nico
                            "count": count,
                            "text": link_text.strip()
                        })
            
            if found_zero:
                break
            
            if len(links_data) == 0:
                break
            
            print(f"‚úÖ {len(links_data)} enlaces para procesar")
            
            # Procesar cada enlace
            for i, link_data in enumerate(links_data, 1):
                print(f"üìä Enlace {i}/{len(links_data)}: {link_data['count']} cuentas")
                
                # Re-obtener enlace por FECHA (inmutable)
                fresh_links = await self.iframe.locator(
                    f"div[id*='panelAPago'] a[onclick*=\"'{link_data['fecha']}'\"]"
                ).all()
                
                if not fresh_links:
                    await self.logger.capture_state(self.page, self.iframe, f"error_no_link_pag{page_num}_iter{i}", {
                        "fecha_buscada": link_data['fecha'],
                        "iteracion": i
                    })
                    continue
                
                target_link = fresh_links[0]
                
                # Click en el enlace
                try:
                    await target_link.click()
                    await asyncio.sleep(3)
                    
                    if await self.iframe.locator("text=Fecha Recepci√≥n Isapre:").count() == 0:
                        continue
                except Exception as e:
                    await self.logger.capture_state(self.page, self.iframe, f"error_click_pag{page_num}_iter{i}", {
                        "error": str(e),
                        "fecha": link_data['fecha']
                    })
                    continue
                
                # Procesar tabla detalle
                try:
                    pdfs = await self.process_detail_table(link_data['text'])
                    all_pdfs.extend(pdfs)
                    
                    # process_detail_table() ya vuelve a tabla resumen
                    # Solo esperamos a que la tabla se estabilice y capturamos estado
                    await asyncio.sleep(2)
                    
                    # Capturar estado despu√©s de volver
                    await self.logger.capture_state(self.page, self.iframe, f"vuelta_pag{page_num}_iter{i}", {
                        "enlace_procesado": link_data['fecha'],
                        "pdfs_descargados": len(pdfs)
                    })
                    
                    print(f"‚úÖ Completado: {len(pdfs)} PDFs")
                    
                except Exception as e:
                    await self.logger.capture_state(self.page, self.iframe, f"error_detalle_pag{page_num}_iter{i}", {
                        "error": str(e)
                    })
                    print(f"‚ùå Error: {e}")
            
            # Paginaci√≥n
            try:
                next_btn = self.iframe.locator("#panelResumen_CallBackPanel_1_gridCuentaMedicaResumen_DXPagerBottom_PBN")
                if await next_btn.count() > 0:
                    await next_btn.click()
                    await asyncio.sleep(2)
                    page_num += 1
                    continue
                else:
                    break
            except:
                break
        
        # Validaci√≥n final
        validation_report = await self.final_validation(all_pdfs)
        
        print(f"\n‚úÖ Total: {len(all_pdfs)} PDFs")
        return all_pdfs, validation_report
    
    async def process_detail_table(self, link_text: str) -> List[Dict[str, Any]]:
        """Procesa la tabla de detalle, extrae metadata completa y descarga todos los PDFs."""
        print(f"    üìã Procesando tabla de detalle...")
        
        # Verificar que aparezca la tabla de detalle
        await self.iframe.locator("text=Fecha Recepci√≥n Isapre:").wait_for(timeout=10000)
        
        # CRITICAL: Resetear paginaci√≥n a p√°gina 1
        # La tabla detalle tiene "memoria" de la p√°gina del enlace anterior
        # DEBUG: Listar todos los botones de paginaci√≥n disponibles
        try:
            all_pager_links = await self.iframe.locator("#panelCuentas_CallBackPanel_gridCuentaMedica_DXPagerBottom a").all()
            print(f"    üîç DEBUG: Botones de paginaci√≥n encontrados: {len(all_pager_links)}")
            for idx, link in enumerate(all_pager_links[:5]):  # Mostrar m√°ximo 5
                text = await link.text_content()
                onclick = await link.get_attribute("onclick")
                print(f"      [{idx}] Texto: '{text.strip()}' | onclick: {onclick[:50] if onclick else 'N/A'}...")
            
            # Buscar el bot√≥n "1" con clase dxp-num
            page_1_btn = self.iframe.locator("a.dxp-num[onclick*='PN0']")
            count = await page_1_btn.count()
            print(f"    üîç Botones con PN0: {count}")
            
            if count > 0:
                await page_1_btn.first.click()
                await asyncio.sleep(2)  # Aumentado de 1 a 2 segundos
                # Esperar que la tabla se actualice
                await self.iframe.locator("#panelCuentas_CallBackPanel_gridCuentaMedica_DXMainTable tr[id*='DXDataRow']").first.wait_for(state="visible", timeout=5000)
                print(f"    üîÑ Paginaci√≥n reseteada a p√°gina 1")
        except Exception as e:
            print(f"    ‚ö†Ô∏è  Error en reset de paginaci√≥n: {e}")
            import traceback
            traceback.print_exc()
        
        # Extraer headers de la tabla (columnas)
        headers_row = await self.iframe.locator("#panelCuentas_CallBackPanel_gridCuentaMedica_DXHeadersRow0 td").all()
        headers = []
        for header in headers_row:
            header_text = await header.text_content()
            # Limpiar texto (quitar saltos de l√≠nea, espacios extras)
            clean_text = " ".join(header_text.strip().split())
            headers.append(clean_text)
        
        print(f"    üìã Columnas: {', '.join(headers)}")
        
        # Lista para almacenar TODOS los datos de la tabla
        all_rows_data = []
        pdfs_data = []
        page_num = 1
        
        while True:
            print(f"üìÑ Procesando p√°gina {page_num} de tabla detalle...")
            
            # Obtener TODAS las filas de datos (excluir header)
            data_rows = await self.iframe.locator("#panelCuentas_CallBackPanel_gridCuentaMedica_DXMainTable tr[id*='DXDataRow']").all()
            
            if not data_rows:
                print("‚ö†Ô∏è  No se encontraron filas en tabla detalle")
                break
            
            print(f"    üìå Encontradas {len(data_rows)} filas en esta p√°gina")
            
            # Procesar cada fila: extraer datos completos y descargar PDF
            for row_idx, data_row in enumerate(data_rows, 1):
                # Extraer TODAS las celdas de la fila
                cells = await data_row.locator("td.dxgv").all()
                
                # Crear diccionario con metadata completa
                row_data = {}
                for idx, cell in enumerate(cells):
                    if idx < len(headers):
                        cell_text = await cell.text_content()
                        row_data[headers[idx]] = cell_text.strip() if cell_text else ""
                
                # Guardar en lista para CSV
                all_rows_data.append(row_data.copy())
                
                # Obtener Nro. Cuenta (primera columna)
                nro_cuenta = row_data.get("Nro. Cuenta", f"row_{row_idx}")
                
                # Buscar enlace de PDF en esta fila
                pdf_link = data_row.locator("a[onclick*='AbrirImagen_ReporteLiquidacion']")
                
                if await pdf_link.count() == 0:
                    print(f"      ‚ö†Ô∏è  Fila {row_idx} (Cuenta {nro_cuenta}): Sin PDF")
                    continue
                
                # Extraer token del PDF
                try:
                    onclick = await pdf_link.get_attribute("onclick")
                    token_match = re.search(r"AbrirImagen_ReporteLiquidacion\('([^']+)'", onclick)
                    if not token_match:
                        print(f"      ‚ö†Ô∏è  Cuenta {nro_cuenta}: No se pudo extraer token")
                        continue
                    
                    token = token_match.group(1)
                    row_data["pdf_token"] = token
                    
                    print(f"      üìÑ [{row_idx}/{len(data_rows)}] Cuenta {nro_cuenta} - Token: {token[:10]}...")
                    
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  Cuenta {nro_cuenta}: Error extrayendo token: {e}")
                    continue
                
                # Descargar PDF con validaci√≥n robusta
                pdf_downloaded = False
                for retry in range(3):
                    try:
                        print(f"      üîÑ Intento {retry + 1}/3: Descargando PDF para {nro_cuenta}")
                        
                        # Generar nombre de archivo √∫nico
                        pdf_filename = f"{nro_cuenta}_{link_text}_{token[:8]}.pdf"
                        pdf_path = self.pdf_dir / self.job_id / pdf_filename
                        pdf_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        # Verificar si ya existe (evitar duplicados)
                        if pdf_path.exists():
                            print(f"      ‚ö†Ô∏è  PDF ya existe: {pdf_filename}")
                            # Verificar que no est√© corrupto
                            if pdf_path.stat().st_size > 0:
                                row_data["pdf_filename"] = pdf_filename
                                row_data["pdf_size"] = pdf_path.stat().st_size
                                row_data["pdf_token"] = token
                                pdfs_data.append(row_data)
                                print(f"      ‚úÖ PDF existente v√°lido: {pdf_filename} ({pdf_path.stat().st_size} bytes)")
                                pdf_downloaded = True
                                break
                            else:
                                print(f"      üóëÔ∏è  PDF corrupto, eliminando: {pdf_filename}")
                                pdf_path.unlink()
                        
                        # Configurar descarga
                        async with self.page.expect_download() as download_info:
                            await pdf_link.click()
                        
                        download = await download_info.value
                        
                        # Guardar con validaci√≥n
                        await download.save_as(pdf_path)
                        
                        # VALIDACI√ìN CR√çTICA: Verificar que el PDF se descarg√≥ correctamente
                        await asyncio.sleep(1)  # Esperar a que se escriba completamente
                        
                        if pdf_path.exists() and pdf_path.stat().st_size > 1000:  # M√≠nimo 1KB
                            row_data["pdf_filename"] = pdf_filename
                            row_data["pdf_size"] = pdf_path.stat().st_size
                            row_data["pdf_token"] = token
                            pdfs_data.append(row_data)
                            print(f"      ‚úÖ PDF descargado y validado: {pdf_filename} ({pdf_path.stat().st_size} bytes)")
                            pdf_downloaded = True
                            break
                        else:
                            print(f"      ‚ö†Ô∏è  PDF descargado pero inv√°lido: {pdf_filename}")
                            if pdf_path.exists():
                                pdf_path.unlink()  # Eliminar archivo corrupto
                            if retry < 2:
                                await asyncio.sleep(2)
                                
                    except Exception as e:
                        print(f"      ‚ö†Ô∏è  Intento {retry + 1}/3 fall√≥: {e}")
                        if retry < 2:
                            await asyncio.sleep(2)
                
                if not pdf_downloaded:
                    print(f"      ‚ùå FALLO CR√çTICO: No se pudo descargar PDF para {nro_cuenta} despu√©s de 3 intentos")
                    # Registrar el fallo para auditor√≠a
                    row_data["download_status"] = "FAILED"
                    row_data["error_count"] = 3
                    pdfs_data.append(row_data)
            
            # Verificar si hay siguiente p√°gina (detectar bot√≥n deshabilitado)
            try:
                next_btn = self.iframe.locator("#panelCuentas_CallBackPanel_gridCuentaMedica_DXPagerBottom_PBN")
                
                # Verificar si el bot√≥n existe y NO est√° deshabilitado
                if await next_btn.count() > 0:
                    # Verificar si tiene clase "dxp-disabledButton" (√∫ltima p√°gina)
                    btn_class = await next_btn.get_attribute("class")
                    if btn_class and "dxp-disabledButton" in btn_class:
                        print(f"    ‚ÑπÔ∏è  √öltima p√°gina detectada (bot√≥n Next deshabilitado)")
                        break
                    
                    # Bot√≥n activo, ir a siguiente p√°gina
                    await next_btn.click()
                    await asyncio.sleep(3)
                    page_num += 1
                    continue
                else:
                    # No hay bot√≥n Next (solo 1 p√°gina)
                    print(f"    ‚ÑπÔ∏è  Solo existe 1 p√°gina")
                    break
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Error en paginaci√≥n: {e}")
                break
        
        # Guardar metadata completa como CSV
        if all_rows_data:
            csv_filename = f"metadata_{link_text}_{self.job_id}.csv"
            csv_path = Path("data/metadata") / csv_filename
            csv_path.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                import csv
                with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                    if all_rows_data:
                        writer = csv.DictWriter(f, fieldnames=headers)
                        writer.writeheader()
                        writer.writerows(all_rows_data)
                print(f"    üíæ Metadata guardada: {csv_filename} ({len(all_rows_data)} filas)")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Error guardando CSV: {e}")
        
        # VALIDACI√ìN CRUZADA Y REINTENTOS
        await self.validate_and_retry_downloads(pdfs_data, link_text)
        
        # RESUMEN FINAL
        total_attempted = len([r for r in pdfs_data])
        total_success = len([r for r in pdfs_data if r.get("pdf_filename")])
        total_failed = len([r for r in pdfs_data if r.get("download_status") == "FAILED"])
        expected_cuentas = int(link_text) if link_text.isdigit() else 0
        
        print(f"\nüìä RESUMEN FINAL - Enlace {link_text}:")
        print(f"   üìÑ Total filas procesadas: {total_attempted}")
        print(f"   ‚úÖ PDFs descargados exitosamente: {total_success}")
        print(f"   ‚ùå PDFs fallidos: {total_failed}")
        
        # Validaci√≥n: PDFs >= Cuentas (una cuenta puede tener m√∫ltiples PDFs)
        if expected_cuentas > 0:
            if total_success < expected_cuentas:
                print(f"   üö® ERROR CR√çTICO: Se esperaban al menos {expected_cuentas} PDFs (cuentas) pero solo se descargaron {total_success}")
                print(f"   ‚ö†Ô∏è  Posible problema: paginaci√≥n empez√≥ en p√°gina incorrecta o PDFs faltantes")
            elif total_success > expected_cuentas:
                print(f"   ‚úÖ OK: {total_success} PDFs de {expected_cuentas} cuentas (algunas cuentas tienen m√∫ltiples PDFs)")
            else:
                print(f"   ‚úÖ PERFECTO: {total_success} PDFs de {expected_cuentas} cuentas")
        
        if total_failed > 0:
            print(f"   ‚ö†Ô∏è  ATENCI√ìN: {total_failed} PDFs no se pudieron descargar despu√©s de todos los reintentos")
        
        # Volver a la tabla resumen usando el bot√≥n de b√∫squeda
        try:
            await self.iframe.locator("#panelCuentas_CallBackPanel_btnSearchImg").click()
            await asyncio.sleep(2)
            print("‚úÖ Volviendo a tabla resumen")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error volviendo a tabla resumen: {e}")
        
        return pdfs_data
    
    async def validate_and_retry_downloads(self, pdfs_data: List[Dict[str, Any]], link_text: str) -> None:
        """Valida que los PDFs descargados coincidan con la metadata y reintenta si no cuadran."""
        print(f"\nüîç VALIDACI√ìN CRUZADA - Enlace {link_text}")
        
        # Contar cu√°ntos deber√≠an haberse descargado
        expected_pdfs = len([r for r in pdfs_data if r.get("pdf_token")])
        actual_pdfs = len([r for r in pdfs_data if r.get("pdf_filename")])
        
        print(f"   üìä Esperados: {expected_pdfs}, Descargados: {actual_pdfs}")
        
        if expected_pdfs == actual_pdfs:
            print(f"   ‚úÖ Todos los PDFs descargados correctamente")
            return
        
        print(f"   ‚ö†Ô∏è  DISCREPANCIA DETECTADA: Faltan {expected_pdfs - actual_pdfs} PDFs")
        
        # Identificar cu√°les fallaron
        failed_records = [r for r in pdfs_data if not r.get("pdf_filename") and r.get("pdf_token")]
        
        if not failed_records:
            print(f"   ‚ÑπÔ∏è  No hay registros fallidos para reintentar")
            return
        
        print(f"   üîÑ REINTENTANDO {len(failed_records)} PDFs fallidos...")
        
        # Reintentar cada PDF fallido
        for i, record in enumerate(failed_records, 1):
            nro_cuenta = record.get('Nro. Cuenta', '')
            print(f"   üîÑ [{i}/{len(failed_records)}] Reintentando: {nro_cuenta}")
            
            # Buscar el enlace PDF en la tabla actual
            try:
                # Buscar fila por Nro. Cuenta
                rows = await self.iframe.locator("table").filter(has_text="Nro. Cuenta").locator("tr").all()
                
                target_row = None
                for row in rows[1:]:  # Skip header
                    cells = await row.locator("td").all()
                    if len(cells) > 0:
                        cell_text = await cells[0].text_content()
                        if cell_text and cell_text.strip() == nro_cuenta:
                            target_row = row
                            break
                
                if not target_row:
                    print(f"      ‚ùå No se encontr√≥ fila para {nro_cuenta}")
                    continue
                
                # Buscar enlace PDF en esa fila
                pdf_link = target_row.get_by_role("link", name="Reporte Liquidaci√≥n").first
                if await pdf_link.count() == 0:
                    print(f"      ‚ùå No se encontr√≥ enlace PDF para {nro_cuenta}")
                    continue
                
                # Reintentar descarga
                success = False
                for retry in range(2):  # Solo 2 reintentos adicionales
                    try:
                        print(f"      üîÑ Reintento {retry + 1}/2 para {nro_cuenta}")
                        
                        # Generar nombre √∫nico
                        token = record.get("pdf_token", "")
                        pdf_filename = f"{nro_cuenta}_{link_text}_{token[:8]}_retry{retry+1}.pdf"
                        pdf_path = self.pdf_dir / self.job_id / pdf_filename
                        
                        # Configurar descarga
                        async with self.page.expect_download() as download_info:
                            await pdf_link.click()
                        
                        download = await download_info.value
                        await download.save_as(pdf_path)
                        
                        # Validar descarga
                        await asyncio.sleep(1)
                        if pdf_path.exists() and pdf_path.stat().st_size > 1000:
                            record["pdf_filename"] = pdf_filename
                            record["pdf_size"] = pdf_path.stat().st_size
                            record["retry_success"] = True
                            record["retry_count"] = retry + 1
                            print(f"      ‚úÖ Reintento exitoso: {pdf_filename} ({pdf_path.stat().st_size} bytes)")
                            success = True
                            break
                        else:
                            print(f"      ‚ö†Ô∏è  Reintento fall√≥: archivo corrupto")
                            if pdf_path.exists():
                                pdf_path.unlink()
                            await asyncio.sleep(1)
                            
                    except Exception as e:
                        print(f"      ‚ùå Error en reintento {retry + 1}: {e}")
                        await asyncio.sleep(1)
                
                if not success:
                    print(f"      ‚ùå Todos los reintentos fallaron para {nro_cuenta}")
                    record["download_status"] = "FAILED_FINAL"
                    record["final_error"] = "All retries failed"
                
            except Exception as e:
                print(f"      ‚ùå Error general en reintento para {nro_cuenta}: {e}")
                record["download_status"] = "FAILED_FINAL"
                record["final_error"] = str(e)
        
        # Resumen final de reintentos
        final_success = len([r for r in pdfs_data if r.get("pdf_filename")])
        print(f"   üìä RESULTADO FINAL: {final_success}/{expected_pdfs} PDFs descargados")
    
    async def final_validation(self, all_pdfs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Validaci√≥n final global de todos los PDFs descargados."""
        print(f"\nüîç VALIDACI√ìN GLOBAL FINAL")
        
        # Contar estad√≠sticas
        total_records = len(all_pdfs)
        successful_downloads = len([r for r in all_pdfs if r.get("pdf_filename")])
        failed_downloads = len([r for r in all_pdfs if r.get("download_status") == "FAILED"])
        retry_successes = len([r for r in all_pdfs if r.get("retry_success")])
        
        print(f"   üìä ESTAD√çSTICAS GLOBALES:")
        print(f"      üìÑ Total registros procesados: {total_records}")
        print(f"      ‚úÖ PDFs descargados exitosamente: {successful_downloads}")
        print(f"      üîÑ PDFs recuperados en reintentos: {retry_successes}")
        print(f"      ‚ùå PDFs fallidos definitivamente: {failed_downloads}")
        
        # Verificar archivos en disco
        corrupted_files = []
        total_size = 0
        pdf_dir = self.pdf_dir / self.job_id
        if pdf_dir.exists():
            pdf_files = list(pdf_dir.glob("*.pdf"))
            print(f"      üìÇ Archivos PDF en disco: {len(pdf_files)}")
            
            # Verificar integridad de archivos
            for pdf_file in pdf_files:
                size = pdf_file.stat().st_size
                total_size += size
                if size < 1000:  # Menos de 1KB probablemente corrupto
                    corrupted_files.append(pdf_file.name)
            
            print(f"      üíæ Tama√±o total descargado: {total_size:,} bytes")
            
            if corrupted_files:
                print(f"      ‚ö†Ô∏è  ARCHIVOS CORRUPTOS DETECTADOS:")
                for file in corrupted_files:
                    print(f"         ‚ùå {file}")
            else:
                print(f"      ‚úÖ Todos los archivos PDF tienen integridad v√°lida")
        
        # Identificar registros problem√°ticos
        failed_records = []
        if failed_downloads > 0:
            print(f"\n   ‚ö†Ô∏è  REGISTROS CON PROBLEMAS:")
            for record in all_pdfs:
                if record.get("download_status") == "FAILED":
                    error_info = {
                        "nro_cuenta": record.get('Nro. Cuenta', 'N/A'),
                        "error": record.get('final_error', 'Unknown error')
                    }
                    failed_records.append(error_info)
                    print(f"      ‚ùå {error_info['nro_cuenta']}: {error_info['error']}")
        
        # Calcular tasa de √©xito
        success_rate = (successful_downloads / total_records * 100) if total_records > 0 else 0
        print(f"\n   üìà TASA DE √âXITO: {success_rate:.1f}%")
        
        validation_passed = success_rate >= 95 and len(corrupted_files) == 0
        
        if success_rate >= 95:
            print(f"   üéØ EXCELENTE: Tasa de √©xito superior al 95%")
        elif success_rate >= 90:
            print(f"   ‚úÖ BUENO: Tasa de √©xito superior al 90%")
        elif success_rate >= 80:
            print(f"   ‚ö†Ô∏è  ACEPTABLE: Tasa de √©xito superior al 80%")
        else:
            print(f"   ‚ùå PROBLEM√ÅTICO: Tasa de √©xito inferior al 80%")
        
        # Retornar reporte estructurado
        return {
            "passed": validation_passed,
            "total_expected": total_records,
            "total_downloaded": successful_downloads,
            "success_rate": round(success_rate, 2),
            "failed_records": failed_records,
            "corrupted_files": corrupted_files,
            "retry_successes": retry_successes,
            "total_size_bytes": total_size
        }
    
    async def download_pdf(self, row_data: Dict[str, Any], link_text: str, pdf_element) -> str:
        """Descarga un PDF espec√≠fico."""
        nro_cuenta = row_data.get("Nro. Cuenta", "unknown")
        
        # Configurar descarga
        download_path = self.pdf_dir / f"{self.job_id}"
        download_path.mkdir(parents=True, exist_ok=True)
        
        # Generar nombre de archivo
        pdf_filename = f"{nro_cuenta}_{link_text}.pdf"
        pdf_path = download_path / pdf_filename
        
        # Configurar download handler
        async with self.page.expect_download() as download_info:
            await pdf_element.click()
        
        download = await download_info.value
        
        # Guardar archivo
        await download.save_as(pdf_path)
        print(f"    üíæ PDF guardado: {pdf_path}")
        
        return pdf_filename
    
    async def discover_documents(self, page: Page, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Flujo principal de descubrimiento y descarga de documentos."""
        print("üöÄ Iniciando descubrimiento de documentos...")
        
        self.page = page
        self.context = page.context
        self.job_id = params.get("job_id", f"job_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        
        # Inicializar logger aut√≥nomo
        self.logger = DebugLogger(self.job_id)
        
        # 1. Navegar a la secci√≥n de cuentas
        await self.navigate_to_accounts_section()
        
        # 2. Aplicar filtros
        has_data = await self.apply_filters(
            prestador=params.get("prestador"),
            year=params.get("year", "2025"),
            month=params.get("month", "SEPTIEMBRE")
        )
        
        if not has_data:
            print("‚ö†Ô∏è  No hay datos para este per√≠odo, retornando lista vac√≠a")
            return []
        
        # 3. Obtener, procesar y descargar TODO (con reintentos y validaciones)
        try:
            all_pdfs, validation_report = await self.get_summary_table_data_and_process()
            
            if not all_pdfs:
                print("‚ö†Ô∏è  No se descargaron PDFs (tabla vac√≠a o sin enlaces v√°lidos)")
                # Guardar resultado vac√≠o con metadata
                results_file = self.results_dir / f"{self.job_id}_results.json"
                results_file.parent.mkdir(parents=True, exist_ok=True)
                results_data = {
                    "job_id": self.job_id,
                    "timestamp": datetime.now().isoformat(),
                    "status": "no_data",
                    "total_pdfs": 0,
                    "pdfs": [],
                    "validation": {
                        "passed": False,
                        "total_expected": 0,
                        "total_downloaded": 0,
                        "success_rate": 0,
                        "failed_records": [],
                        "corrupted_files": []
                    }
                }
                results_file.write_text(json.dumps(results_data, indent=2, ensure_ascii=False))
                return []
            
            # Guardar resultados exitosos
            results_file = self.results_dir / f"{self.job_id}_results.json"
            results_file.parent.mkdir(parents=True, exist_ok=True)
            results_data = {
                "job_id": self.job_id,
                "timestamp": datetime.now().isoformat(),
                "status": "success",
                "total_pdfs": len(all_pdfs),
                "pdfs": all_pdfs,
                "validation": validation_report
            }
            
            results_file.write_text(json.dumps(results_data, indent=2, ensure_ascii=False))
            
            # Guardar reportes de debug
            self.logger.save_report()
            analysis = self.logger.analyze_and_save()
            
            print(f"\nüéâ SCRAPING COMPLETADO")
            print(f"üìä Total: {len(all_pdfs)} PDFs descargados")
            print(f"üìÅ Resultados: {results_file}")
            print(f"üìÇ PDFs: {self.pdf_dir / self.job_id}")
            print(f"üîç Debug: data/debug/{self.job_id}/")
            
            if analysis:
                print(f"üìà An√°lisis: {analysis['total_eventos']} eventos, {analysis['enlaces_procesados']} enlaces procesados")
                if analysis['errores']:
                    print(f"‚ö†Ô∏è  {len(analysis['errores'])} errores detectados (ver analysis.json)")
            
            return all_pdfs
            
        except Exception as e:
            print(f"‚ùå Error fatal en scraping: {e}")
            import traceback
            traceback.print_exc()
            
            # Guardar reportes de debug incluso en caso de error
            if self.logger:
                self.logger.save_report()
                self.logger.analyze_and_save()
                print(f"üîç Debug info guardado en: data/debug/{self.job_id}/")
            
            # Guardar resultado de error
            results_file = self.results_dir / f"{self.job_id}_results.json"
            results_file.parent.mkdir(parents=True, exist_ok=True)
            results_data = {
                "job_id": self.job_id,
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "error": str(e),
                "total_pdfs": 0,
                "pdfs": []
            }
            results_file.write_text(json.dumps(results_data, indent=2, ensure_ascii=False))
            
            return []
    
    def extract(self, pdf_path: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extrae informaci√≥n de un PDF descargado."""
        # TODO: Implementar extracci√≥n de PDFs usando pdfplumber o similar
        # Por ahora retorna metadata b√°sico
        return [metadata]
    
    def postprocess(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Post-procesamiento de registros extra√≠dos."""
        # TODO: Implementar normalizaci√≥n de datos
        return records
